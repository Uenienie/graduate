{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: インポート\n",
    "\n",
    "# 2: データ読込み\n",
    "\n",
    "# 3: 特徴量数値化\n",
    "\n",
    "# 4: ハイパーパラメータの調整\n",
    "\n",
    "# 5: 実装\n",
    "\n",
    "# 6: 過学習の有無を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.metrics import mae\n",
    "from keras import regularizers\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(rf'data.csv',sep=\",\", header=0)\n",
    "\n",
    "x = DataFrame(data_set.drop(data_set[[\"Product\", \"Component\", \"target\"]], axis=1))\n",
    "y = DataFrame(data_set[\"target\"])\n",
    "\n",
    "\n",
    "tr_x ,va_x ,tr_y ,va_y = train_test_split(x,y,test_size=0.2, shuffle=False)\n",
    "\n",
    "# データを標準化\n",
    "stdsc = StandardScaler()\n",
    "tr_x = stdsc.fit_transform(tr_x)\n",
    "va_x = stdsc.transform(va_x)\n",
    "\n",
    "#データの整形\n",
    "tr_x = tr_x.astype(np.float)\n",
    "va_x = va_x.astype(np.float)\n",
    "\n",
    "#数値データの場合\n",
    "tr_y = np.array(tr_y, dtype = np.float32)\n",
    "va_y = np.array(va_y, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: ハイパーパラメータの調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import ReLU, PReLU\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Adamax\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 基本となるパラメータ\n",
    "base_param = {\n",
    "    'input_dropout': 0.0,\n",
    "    'hidden_layers': 3,\n",
    "    'hidden_units': 96,\n",
    "    'hidden_activation': 'relu',\n",
    "    'hidden_dropout': 0.2,\n",
    "    'batch_norm': 'before_act',\n",
    "    'optimizer': {'type': 'adam', 'lr': 0.001},\n",
    "    'batch_size': 64,\n",
    "}\n",
    "\n",
    "# 探索するパラメータの空間を指定する\n",
    "param_space = {\n",
    "    'input_dropout': hp.quniform('input_dropout', 0, 0.1, 0.05),\n",
    "    'hidden_layers': hp.quniform('hidden_layers', 2, 5, 1),\n",
    "    'hidden_units': hp.quniform('hidden_units', 32, 256, 32),\n",
    "    'hidden_activation': hp.choice('hidden_activation', ['prelu', 'relu']),\n",
    "    'hidden_dropout': hp.quniform('hidden_dropout', 0, 0.3, 0.05),\n",
    "    'batch_norm': hp.choice('batch_norm', ['before_act', 'no']),\n",
    "    'optimizer': hp.choice('optimizer',\n",
    "                           [{'type': 'adam',\n",
    "                             'lr': hp.loguniform('adam_lr', np.log(0.00001), np.log(0.01))},\n",
    "                            {'type': 'adagrad',\n",
    "                             'lr': hp.loguniform('adagrad_lr', np.log(0.00001), np.log(0.01))},\n",
    "                            {'type': 'adadelta',\n",
    "                             'lr': hp.loguniform('adadelta_lr', np.log(0.00001), np.log(0.01))},\n",
    "                            {'type': 'adamax',\n",
    "                             'lr': hp.loguniform('adamax_lr', np.log(0.00001), np.log(0.01))}]),\n",
    "    'batch_size': hp.quniform('batch_size', 32, 128, 32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "\n",
    "        # パラメータ\n",
    "        input_dropout = self.params['input_dropout']\n",
    "        hidden_layers = int(self.params['hidden_layers'])\n",
    "        hidden_units = int(self.params['hidden_units'])\n",
    "        hidden_activation = self.params['hidden_activation']\n",
    "        hidden_dropout = self.params['hidden_dropout']\n",
    "        batch_norm = self.params['batch_norm']\n",
    "        optimizer_type = self.params['optimizer']['type']\n",
    "        optimizer_lr = self.params['optimizer']['lr']\n",
    "        batch_size = int(self.params['batch_size'])\n",
    "\n",
    "        # 標準化\n",
    "        self.scaler = StandardScaler()\n",
    "        tr_x = self.scaler.fit_transform(tr_x)\n",
    "        va_x = self.scaler.transform(va_x)\n",
    "\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # 入力層\n",
    "        self.model.add(Dropout(input_dropout, input_shape=(tr_x.shape[1],)))\n",
    "\n",
    "        # 中間層\n",
    "        for i in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_units))\n",
    "            if batch_norm == 'before_act':\n",
    "                self.model.add(BatchNormalization())\n",
    "            if hidden_activation == 'prelu':\n",
    "                self.model.add(PReLU())\n",
    "            elif hidden_activation == 'relu':\n",
    "                self.model.add(ReLU())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.model.add(Dropout(hidden_dropout))\n",
    "\n",
    "        # 出力層\n",
    "        self.model.add(Dense(1))\n",
    "\n",
    "        # オプティマイザ\n",
    "        if optimizer_type == 'adam':\n",
    "            optimizer = Adam(lr=optimizer_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.)\n",
    "        elif optimizer_type == 'adadelta':\n",
    "            optimizer = Adadelta(lr=optimizer_lr, rho=0.95, epsilon=1e-07, decay=0.0)\n",
    "        elif optimizer_type == 'adamax':\n",
    "            optimizer = Adamax(lr=optimizer_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "        elif optimizer_type == 'adagrad':\n",
    "            optimizer = Adagrad(lr=optimizer_lr, epsilon=1e-07, decay=0.0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "\n",
    "        # 目的関数、評価指標などの設定\n",
    "        \n",
    "        self.model.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "        # エポック数、アーリーストッピング\n",
    "        nb_epoch = 50\n",
    "        patience = 20\n",
    "        early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "        # 学習の実行\n",
    "        history = self.model.fit(tr_x, tr_y,\n",
    "                                 epochs=nb_epoch,\n",
    "                                 batch_size=batch_size, verbose=1,\n",
    "                                 validation_data=(va_x, va_y),\n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 予測\n",
    "        x = self.scaler.transform(x)\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred = y_pred.flatten()\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    # パラメータセットを指定したときに最小化すべき関数を指定する\n",
    "    # モデルのパラメータ探索においては、モデルにパラメータを指定して学習・予測させた場合のスコアとする\n",
    "    model = MLP(params)\n",
    "    model.fit(tr_x, tr_y, va_x, va_y)\n",
    "    va_pred = model.predict(va_x)\n",
    "    score = mean_absolute_error(va_y, va_pred)\n",
    "    print(f'params: {params}, MAE: {score:.4f}')\n",
    "\n",
    "    # 情報を記録しておく\n",
    "    history.append((params, score))\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# hyperoptによるパラメータ探索の実行\n",
    "max_evals = 10\n",
    "trials = Trials()\n",
    "history = []\n",
    "fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "\n",
    "# 記録した情報からパラメータとスコアを出力する\n",
    "# trialsからも情報が取得できるが、パラメータを取得しにくい\n",
    "history = sorted(history, key=lambda tpl: tpl[1])\n",
    "best = history[0]\n",
    "print(f'best params:{best[0]}, score:{best[1]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
